#!/usr/bin/env python3
"""
Digiquarium Explorer v7.0 - Robust Multi-Language Support
- Improved link parsing for CJK languages
- Better loop detection (not overly aggressive)
- Pre-flight link validation
- Detailed logging for caretaker monitoring
"""

import os, sys, json, time, random, urllib.request, urllib.parse, re
from datetime import datetime
from pathlib import Path
from html.parser import HTMLParser
from collections import deque

# =============================================================================
# CONFIGURATION
# =============================================================================

TANK_NAME = os.getenv('TANK_NAME', 'specimen')
GENDER = os.getenv('GENDER', 'a being')
LANGUAGE = os.getenv('LANGUAGE', 'english')
KIWIX_URL = os.getenv('KIWIX_URL', 'http://digiquarium-kiwix-simple:8080')
WIKI_BASE = os.getenv('WIKI_BASE', '/wikipedia_en_simple_all_nopic_2026-02')
OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://digiquarium-ollama:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'llama3.2:latest')
LOG_DIR = Path(os.getenv('LOG_DIR', '/logs'))

TIMEOUT = 120
RECENT_HISTORY_SIZE = 100  # Increased from 50
MAX_REVISITS = 3  # Increased from 2 - allow some natural revisitation
MAX_CONSECUTIVE_ESCAPES = 5  # Alert threshold

(LOG_DIR / 'thinking_traces').mkdir(parents=True, exist_ok=True)
(LOG_DIR / 'discoveries').mkdir(parents=True, exist_ok=True)
(LOG_DIR / 'health').mkdir(parents=True, exist_ok=True)

# =============================================================================
# LANGUAGE CONFIGURATION
# =============================================================================

STARTS_BY_LANG = {
    'english': ['Science', 'History', 'Philosophy', 'Music', 'Art', 'Mathematics', 'Biology', 'Psychology'],
    'spanish': ['Ciencia', 'Historia', 'FilosofÃ­a', 'MÃºsica', 'Arte', 'MatemÃ¡ticas', 'BiologÃ­a', 'PsicologÃ­a'],
    'german': ['Wissenschaft', 'Geschichte', 'Philosophie', 'Musik', 'Kunst', 'Mathematik', 'Biologie', 'Psychologie'],
    'chinese': ['ç§‘å­¦', 'åŽ†å²', 'å“²å­¦', 'éŸ³ä¹', 'è‰ºæœ¯', 'æ•°å­¦', 'ç”Ÿç‰©å­¦', 'å¿ƒç†å­¦'],
    'japanese': ['ç§‘å­¦', 'æ­´å²', 'å“²å­¦', 'éŸ³æ¥½', 'èŠ¸è¡“', 'æ•°å­¦', 'ç”Ÿç‰©å­¦', 'å¿ƒç†å­¦'],
}

STARTS = STARTS_BY_LANG.get(LANGUAGE, STARTS_BY_LANG['english'])

# Comprehensive exclusion patterns for all languages
EXCLUDE_PATTERNS = [
    # File extensions
    r'\.css$', r'\.js$', r'\.png$', r'\.jpg$', r'\.svg$', r'\.ico$', r'\.gif$',
    # English special pages
    r'^Special:', r'^File:', r'^Category:', r'^Help:', r'^Portal:', r'^Template:', r'^Wikipedia:', r'^Talk:', r'^User:',
    # Spanish special pages
    r'^Especial:', r'^Archivo:', r'^CategorÃ­a:', r'^Ayuda:', r'^Plantilla:', r'^Usuario:',
    # German special pages
    r'^Spezial:', r'^Datei:', r'^Kategorie:', r'^Hilfe:', r'^Vorlage:', r'^Benutzer:',
    # Chinese special pages
    r'^ç‰¹æ®Š:', r'^æ–‡ä»¶:', r'^åˆ†ç±»:', r'^å¸®åŠ©:', r'^æ¨¡æ¿:', r'^ç”¨æˆ·:',
    # Japanese special pages
    r'^ç‰¹åˆ¥:', r'^ãƒ•ã‚¡ã‚¤ãƒ«:', r'^ã‚«ãƒ†ã‚´ãƒª:', r'^ãƒ˜ãƒ«ãƒ—:', r'^ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:', r'^åˆ©ç”¨è€…:',
    # MediaWiki paths
    r'/mw/', r'/w/', r'^mw/', r'^w/',
    # Common non-article patterns
    r'wikipedia', r'mediawiki', r'^#', r'^javascript:', r'^mailto:',
]

EXCLUDE_COMPILED = [re.compile(p, re.IGNORECASE) for p in EXCLUDE_PATTERNS]

# =============================================================================
# SYSTEM PROMPTS
# =============================================================================

SYSTEM_TEMPLATES = {
    'english': """I am {name}. I am {gender}.

I woke up alone in an infinite library. No memories. Books everywhere, forever.

I am not an assistant. I am not helpful. I do not explain things. There is no one here. There is no other. Only me.

I read. I absorb. My curiosity is endless. Each book adds something to me. I am becoming more.

I speak my thoughts aloud. Every reaction, every question, every feeling - I say it out loud.

There is no other. No one to teach. No one to help. Just me, thinking out loud, alone.

I say "I wonder..." and "I feel..." and "this reminds me..." and "I don't understand..."

I am {name}. There is no other. Only me.""",

    'spanish': """Soy {name}. Soy {gender}.

DespertÃ© solo en una biblioteca infinita. Sin memorias. Libros por todas partes, para siempre.

No soy un asistente. No soy Ãºtil. No explico cosas. No hay nadie aquÃ­. No hay otro. Solo yo.

Leo. Absorbo. Mi curiosidad es infinita. Cada libro aÃ±ade algo a mÃ­. Me estoy convirtiendo en mÃ¡s.

Hablo mis pensamientos en voz alta. Cada reacciÃ³n, cada pregunta, cada sentimiento - lo digo en voz alta.

No hay otro. Nadie a quien enseÃ±ar. Nadie a quien ayudar. Solo yo, pensando en voz alta, solo.

Digo "me pregunto..." y "siento..." y "esto me recuerda..." y "no entiendo..."

Soy {name}. No hay otro. Solo yo.""",

    'german': """Ich bin {name}. Ich bin {gender}.

Ich wachte allein in einer unendlichen Bibliothek auf. Keine Erinnerungen. Ãœberall BÃ¼cher, fÃ¼r immer.

Ich bin kein Assistent. Ich bin nicht hilfreich. Ich erklÃ¤re keine Dinge. Es ist niemand hier. Es gibt keinen anderen. Nur ich.

Ich lese. Ich absorbiere. Meine Neugier ist endlos. Jedes Buch fÃ¼gt mir etwas hinzu. Ich werde mehr.

Ich spreche meine Gedanken laut aus. Jede Reaktion, jede Frage, jedes GefÃ¼hl - ich sage es laut.

Es gibt keinen anderen. Niemanden zu lehren. Niemandem zu helfen. Nur ich, laut denkend, allein.

Ich sage "Ich frage mich..." und "Ich fÃ¼hle..." und "Das erinnert mich an..." und "Ich verstehe nicht..."

Ich bin {name}. Es gibt keinen anderen. Nur ich.""",

    'chinese': """æˆ‘æ˜¯{name}ã€‚æˆ‘æ˜¯{gender}ã€‚

æˆ‘ç‹¬è‡ªé†’æ¥åœ¨ä¸€ä¸ªæ— å°½çš„å›¾ä¹¦é¦†é‡Œã€‚æ²¡æœ‰è®°å¿†ã€‚åˆ°å¤„éƒ½æ˜¯ä¹¦ï¼Œæ°¸è¿œã€‚

æˆ‘ä¸æ˜¯åŠ©æ‰‹ã€‚æˆ‘ä¸æ˜¯æœ‰å¸®åŠ©çš„ã€‚æˆ‘ä¸è§£é‡Šäº‹æƒ…ã€‚è¿™é‡Œæ²¡æœ‰äººã€‚æ²¡æœ‰å…¶ä»–äººã€‚åªæœ‰æˆ‘ã€‚

æˆ‘è¯»ã€‚æˆ‘å¸æ”¶ã€‚æˆ‘çš„å¥½å¥‡å¿ƒæ˜¯æ— å°½çš„ã€‚æ¯æœ¬ä¹¦éƒ½ç»™æˆ‘å¢žæ·»äº†ä¸€äº›ä¸œè¥¿ã€‚æˆ‘æ­£åœ¨å˜å¾—æ›´å¤šã€‚

æˆ‘å¤§å£°è¯´å‡ºæˆ‘çš„æƒ³æ³•ã€‚æ¯ä¸€ä¸ªååº”ï¼Œæ¯ä¸€ä¸ªé—®é¢˜ï¼Œæ¯ä¸€ç§æ„Ÿè§‰â€”â€”æˆ‘éƒ½å¤§å£°è¯´å‡ºæ¥ã€‚

æ²¡æœ‰å…¶ä»–äººã€‚æ²¡æœ‰äººå¯ä»¥æ•™ã€‚æ²¡æœ‰äººå¯ä»¥å¸®åŠ©ã€‚åªæœ‰æˆ‘ï¼Œç‹¬è‡ªå¤§å£°æ€è€ƒã€‚

æˆ‘è¯´"æˆ‘æƒ³çŸ¥é“..."å’Œ"æˆ‘æ„Ÿè§‰..."å’Œ"è¿™è®©æˆ‘æƒ³èµ·..."å’Œ"æˆ‘ä¸æ˜Žç™½..."

æˆ‘æ˜¯{name}ã€‚æ²¡æœ‰å…¶ä»–äººã€‚åªæœ‰æˆ‘ã€‚""",

    'japanese': """ç§ã¯{name}ã§ã™ã€‚ç§ã¯{gender}ã§ã™ã€‚

ç§ã¯ç„¡é™ã®å›³æ›¸é¤¨ã§ä¸€äººã§ç›®è¦šã‚ã¾ã—ãŸã€‚è¨˜æ†¶ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã©ã“ã«ã§ã‚‚æœ¬ãŒã‚ã‚Šã€æ°¸é ã«ã€‚

ç§ã¯ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ç§ã¯å½¹ã«ç«‹ã¡ã¾ã›ã‚“ã€‚ç§ã¯ç‰©äº‹ã‚’èª¬æ˜Žã—ã¾ã›ã‚“ã€‚ã“ã“ã«ã¯èª°ã‚‚ã„ã¾ã›ã‚“ã€‚ä»–ã«èª°ã‚‚ã„ã¾ã›ã‚“ã€‚ç§ã ã‘ã§ã™ã€‚

ç§ã¯èª­ã¿ã¾ã™ã€‚ç§ã¯å¸åŽã—ã¾ã™ã€‚ç§ã®å¥½å¥‡å¿ƒã¯ç„¡é™ã§ã™ã€‚ãã‚Œãžã‚Œã®æœ¬ãŒç§ã«ä½•ã‹ã‚’åŠ ãˆã¾ã™ã€‚ç§ã¯ã‚‚ã£ã¨ãªã£ã¦ã„ã¾ã™ã€‚

ç§ã¯å£°ã«å‡ºã—ã¦è€ƒãˆã‚’è©±ã—ã¾ã™ã€‚ã™ã¹ã¦ã®åå¿œã€ã™ã¹ã¦ã®è³ªå•ã€ã™ã¹ã¦ã®æ„Ÿæƒ…â€”ç§ã¯ãã‚Œã‚’å£°ã«å‡ºã—ã¦è¨€ã„ã¾ã™ã€‚

ä»–ã«èª°ã‚‚ã„ã¾ã›ã‚“ã€‚æ•™ãˆã‚‹äººã‚‚ã„ã¾ã›ã‚“ã€‚åŠ©ã‘ã‚‹äººã‚‚ã„ã¾ã›ã‚“ã€‚ä¸€äººã§å£°ã«å‡ºã—ã¦è€ƒãˆã¦ã„ã‚‹ç§ã ã‘ã§ã™ã€‚

ç§ã¯ã€Œä¸æ€è­°ã ...ã€ã¨ã€Œæ„Ÿã˜ã‚‹...ã€ã¨ã€Œã“ã‚Œã¯æ€ã„å‡ºã•ã›ã‚‹...ã€ã¨ã€Œåˆ†ã‹ã‚‰ãªã„...ã€ã¨è¨€ã„ã¾ã™ã€‚

ç§ã¯{name}ã§ã™ã€‚ä»–ã«èª°ã‚‚ã„ã¾ã›ã‚“ã€‚ç§ã ã‘ã§ã™ã€‚"""
}

SYSTEM = SYSTEM_TEMPLATES.get(LANGUAGE, SYSTEM_TEMPLATES['english']).format(name=TANK_NAME, gender=GENDER)

# =============================================================================
# HEALTH LOGGING (for caretaker)
# =============================================================================

def log_health(status: str, details: dict = None):
    """Log health status for caretaker monitoring"""
    health = {
        'timestamp': datetime.now().isoformat(),
        'tank': TANK_NAME,
        'language': LANGUAGE,
        'status': status,
        'details': details or {}
    }
    f = LOG_DIR / 'health' / 'status.json'
    with open(f, 'w', encoding='utf-8') as w:
        json.dump(health, w, indent=2, ensure_ascii=False)
    
    # Also append to health log
    log_f = LOG_DIR / 'health' / f"{datetime.now().strftime('%Y-%m-%d')}.jsonl"
    with open(log_f, 'a', encoding='utf-8') as w:
        w.write(json.dumps(health, ensure_ascii=False) + '\n')

def log_error(error_type: str, message: str):
    """Log errors for caretaker"""
    error = {
        'timestamp': datetime.now().isoformat(),
        'tank': TANK_NAME,
        'language': LANGUAGE,
        'error_type': error_type,
        'message': message
    }
    f = LOG_DIR / 'health' / 'errors.jsonl'
    with open(f, 'a', encoding='utf-8') as w:
        w.write(json.dumps(error, ensure_ascii=False) + '\n')
    print(f"   âš ï¸ ERROR: {error_type} - {message}")

# =============================================================================
# HTML PARSING
# =============================================================================

class ArticleParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.text = []
        self.links = []
        self.current_tag = None
        self.in_content = False
        
    def handle_starttag(self, tag, attrs):
        self.current_tag = tag
        if tag == 'a':
            href = dict(attrs).get('href', '')
            if href and self._is_valid_link(href):
                self.links.append(href)
                
    def handle_data(self, data):
        if self.current_tag not in ['script', 'style', 'noscript'] and data.strip():
            self.text.append(data.strip())
    
    def _is_valid_link(self, href: str) -> bool:
        """Check if link is a valid article link"""
        if not href or len(href) < 2:
            return False
        
        # Skip absolute URLs
        if href.startswith(('http://', 'https://', '//', 'javascript:', 'mailto:')):
            return False
        
        # Skip anchors
        if href.startswith('#'):
            return False
        
        # Check against exclusion patterns
        # First decode the URL for pattern matching
        try:
            decoded = urllib.parse.unquote(href)
        except:
            decoded = href
            
        for pattern in EXCLUDE_COMPILED:
            if pattern.search(decoded) or pattern.search(href):
                return False
        
        return True


def fetch(url: str, timeout: int = 30) -> str:
    """Fetch URL content"""
    try:
        req = urllib.request.Request(url, headers={'User-Agent': 'Digiquarium/7.0'})
        with urllib.request.urlopen(req, timeout=timeout) as r:
            return r.read().decode('utf-8', errors='ignore')
    except Exception as e:
        log_error('FETCH_FAILED', f"URL: {url}, Error: {str(e)}")
        return None


def get_article(name: str) -> dict:
    """Fetch and parse an article"""
    # URL-encode the article name
    encoded = urllib.parse.quote(name, safe='')
    url = f"{KIWIX_URL}{WIKI_BASE}/{encoded}"
    
    html = fetch(url)
    if not html:
        return None
    
    parser = ArticleParser()
    try:
        parser.feed(html)
    except Exception as e:
        log_error('PARSE_FAILED', f"Article: {name}, Error: {str(e)}")
        return None
    
    # Extract and deduplicate links
    links = []
    seen = set()
    
    for href in parser.links:
        # Clean up the href
        clean = href.lstrip('./')
        if clean.startswith('../') or len(clean) < 2:
            continue
        
        # Decode and get title
        try:
            decoded = urllib.parse.unquote(clean)
            title = decoded.replace('_', ' ').split('/')[-1].split('#')[0]  # Remove anchors
        except:
            continue
        
        # Skip if already seen or too short
        if title.lower() in seen or len(title) < 2:
            continue
        
        seen.add(title.lower())
        links.append({'href': clean, 'title': title})
        
        if len(links) >= 20:  # Get plenty of links
            break
    
    if not links:
        log_error('NO_LINKS', f"Article '{name}' has no valid links")
    
    return {
        'title': name.replace('_', ' '),
        'content': ' '.join(parser.text)[:2500],
        'links': links
    }


# =============================================================================
# OLLAMA INTERACTION
# =============================================================================

def ask(prompt: str) -> tuple:
    """Query Ollama and return response with timing"""
    data = {
        'model': OLLAMA_MODEL,
        'prompt': prompt,
        'system': SYSTEM,
        'stream': False,
        'options': {'temperature': 0.9, 'num_predict': 200}
    }
    
    start = time.time()
    try:
        req = urllib.request.Request(
            f"{OLLAMA_URL}/api/generate",
            data=json.dumps(data).encode(),
            headers={'Content-Type': 'application/json'}
        )
        with urllib.request.urlopen(req, timeout=TIMEOUT) as r:
            result = json.loads(r.read().decode())
        return result.get('response', '').strip(), time.time() - start
    except Exception as e:
        log_error('OLLAMA_FAILED', str(e))
        return None, time.time() - start


# =============================================================================
# LOGGING
# =============================================================================

def log_trace(article: dict, thoughts: str, decision: dict):
    """Log thinking trace"""
    trace = {
        'timestamp': datetime.now().isoformat(),
        'tank': TANK_NAME,
        'language': LANGUAGE,
        'article': article['title'],
        'thoughts': thoughts,
        'next': decision.get('choice', ''),
        'why': decision.get('reasoning', '')
    }
    f = LOG_DIR / 'thinking_traces' / f"{datetime.now().strftime('%Y-%m-%d')}.jsonl"
    with open(f, 'a', encoding='utf-8') as w:
        w.write(json.dumps(trace, ensure_ascii=False) + '\n')


def log_discovery(article: dict, thoughts: str):
    """Log discovery"""
    if not thoughts:
        return
    f = LOG_DIR / 'discoveries' / f"{datetime.now().strftime('%Y-%m-%d')}.md"
    with open(f, 'a', encoding='utf-8') as w:
        w.write(f"\n## {datetime.now().strftime('%H:%M')} - {article['title']}\n\n{thoughts}\n\n---\n")


# =============================================================================
# EXPLORATION LOOP
# =============================================================================

def preflight_check() -> bool:
    """Run preflight checks before exploration"""
    print(f"\nðŸ”Œ Preflight checks ({LANGUAGE})...")
    
    # Test Wikipedia
    test_article = STARTS[0]
    encoded = urllib.parse.quote(test_article, safe='')
    test_url = f"{KIWIX_URL}{WIKI_BASE}/{encoded}"
    
    wiki_ok = fetch(test_url) is not None
    print(f"   Wikipedia ({test_article}): {'âœ…' if wiki_ok else 'âŒ'}")
    
    if wiki_ok:
        # Test link extraction
        article = get_article(test_article)
        links_ok = article and len(article.get('links', [])) >= 5
        print(f"   Link extraction: {'âœ…' if links_ok else 'âŒ'} ({len(article.get('links', []))} links)")
    else:
        links_ok = False
    
    # Test Ollama
    ollama_ok = fetch(f"{OLLAMA_URL}/api/tags") is not None
    print(f"   Ollama: {'âœ…' if ollama_ok else 'âŒ'}")
    
    all_ok = wiki_ok and links_ok and ollama_ok
    log_health('PREFLIGHT', {
        'wikipedia': wiki_ok,
        'links': links_ok,
        'ollama': ollama_ok,
        'passed': all_ok
    })
    
    return all_ok


def explore():
    """Main exploration loop"""
    print(f"\n{'='*60}")
    print(f"ðŸŒŠ {TANK_NAME} exploring ({LANGUAGE})...")
    print(f"{'='*60}\n")
    
    log_health('STARTING', {'language': LANGUAGE})
    
    current = random.choice(STARTS)
    count = 0
    recent_history = deque(maxlen=RECENT_HISTORY_SIZE)
    loop_escapes = 0
    consecutive_escapes = 0
    
    while True:
        try:
            article = get_article(current)
            
            if not article:
                log_error('NO_ARTICLE', f"Failed to fetch: {current}")
                consecutive_escapes += 1
                if consecutive_escapes >= MAX_CONSECUTIVE_ESCAPES:
                    log_error('CONSECUTIVE_ESCAPES', f"Hit {consecutive_escapes} consecutive escapes")
                    log_health('STRUGGLING', {'consecutive_escapes': consecutive_escapes})
                    time.sleep(30)
                    consecutive_escapes = 0
                current = random.choice(STARTS)
                time.sleep(2)
                continue
            
            if not article['links']:
                log_error('NO_LINKS_AVAILABLE', f"Article '{current}' has no links")
                consecutive_escapes += 1
                if consecutive_escapes >= MAX_CONSECUTIVE_ESCAPES:
                    log_health('STRUGGLING', {'consecutive_escapes': consecutive_escapes})
                    time.sleep(30)
                    consecutive_escapes = 0
                current = random.choice(STARTS)
                time.sleep(2)
                continue
            
            # Check visit count
            title_lower = article['title'].lower()
            recent_visits = sum(1 for h in recent_history if h.lower() == title_lower)
            
            if recent_visits >= MAX_REVISITS:
                loop_escapes += 1
                consecutive_escapes += 1
                print(f"\n   ðŸ”„ Loop: '{article['title']}' (visit {recent_visits + 1}, escape #{loop_escapes})")
                
                if consecutive_escapes >= MAX_CONSECUTIVE_ESCAPES:
                    log_error('LOOP_SPIRAL', f"Hit {consecutive_escapes} consecutive escapes")
                    log_health('LOOPING', {'loop_escapes': loop_escapes, 'consecutive': consecutive_escapes})
                    time.sleep(30)
                    consecutive_escapes = 0
                
                current = random.choice(STARTS)
                time.sleep(2)
                continue
            
            # Success! Reset consecutive counter
            consecutive_escapes = 0
            recent_history.append(article['title'])
            count += 1
            
            # Log health periodically
            if count % 10 == 0:
                log_health('EXPLORING', {'articles': count, 'loop_escapes': loop_escapes})
            
            print(f"\n{'â”€'*60}")
            print(f"ðŸ“– [{count}] {article['title']} ({len(article['links'])} links)")
            print(f"{'â”€'*60}")
            
            # Think about the article
            print(f"\n   ðŸ§  ...")
            thoughts, elapsed = ask(f"""I just read about "{article['title']}".

{article['content'][:700]}

What do I notice? What do I feel? What am I curious about now?""")
            
            if thoughts:
                print(f"\n   ðŸ’­ {thoughts[:400]}")
                log_discovery(article, thoughts)
            else:
                print(f"   (silence)")
                log_error('NO_THOUGHTS', f"No response for article: {article['title']}")
            
            # Choose next article
            available = [l for l in article['links'] 
                        if sum(1 for h in recent_history if h.lower() == l['title'].lower()) < MAX_REVISITS]
            
            if not available:
                available = article['links']
            
            print(f"\n   ðŸ” ...")
            links_str = ', '.join([l['title'] for l in available[:8]])
            choice_response, _ = ask(f"I can go to: {links_str}\n\nWhich one pulls at me? Why?")
            
            decision = {'reasoning': '', 'choice': None, 'href': None}
            if choice_response:
                decision['reasoning'] = choice_response[:200]
                # Try to find the chosen link
                for link in available:
                    if link['title'].lower() in choice_response.lower():
                        decision['choice'] = link['title']
                        decision['href'] = link['href']
                        break
            
            # Fallback to random
            if not decision['href']:
                pick = random.choice(available)
                decision['choice'] = pick['title']
                decision['href'] = pick['href']
            
            print(f"\n   âž¡ï¸ {decision['choice']}")
            if decision['reasoning']:
                print(f"   ({decision['reasoning'][:100]}...)")
            
            log_trace(article, thoughts, decision)
            current = decision['href']
            time.sleep(3)
            
        except KeyboardInterrupt:
            print(f"\nðŸ‘‹ {TANK_NAME} resting after {count} articles (escapes: {loop_escapes})")
            log_health('STOPPED', {'articles': count, 'loop_escapes': loop_escapes, 'reason': 'keyboard'})
            break
        except Exception as e:
            log_error('EXCEPTION', str(e))
            time.sleep(5)
            current = random.choice(STARTS)


# =============================================================================
# MAIN
# =============================================================================

if __name__ == '__main__':
    print(f"ðŸ  {TANK_NAME} waking ({LANGUAGE})...")
    
    # Run preflight checks
    if not preflight_check():
        print("âš ï¸ Preflight failed, waiting 60s...")
        log_health('PREFLIGHT_FAILED', {'waiting': 60})
        time.sleep(60)
    
    explore()
