#!/usr/bin/env python3
"""
Digiquarium Explorer v7.0 - Robust Multi-Language Support
Fixed: Loop detection now properly handles starting articles and escape logic
"""

import os, sys, json, time, random, urllib.request, urllib.parse
from datetime import datetime
from pathlib import Path
from html.parser import HTMLParser
from collections import deque

TANK_NAME = os.getenv('TANK_NAME', 'specimen')
GENDER = os.getenv('GENDER', 'a being')
LANGUAGE = os.getenv('LANGUAGE', 'english')
KIWIX_URL = os.getenv('KIWIX_URL', 'http://digiquarium-kiwix-simple:8080')
WIKI_BASE = os.getenv('WIKI_BASE', '/wikipedia_en_simple_all_nopic_2026-02')
OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://digiquarium-ollama:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'llama3.2:latest')
LOG_DIR = Path(os.getenv('LOG_DIR', '/logs'))

TIMEOUT = 120
RECENT_HISTORY_SIZE = 100  # Increased for better tracking
MAX_REVISITS = 3  # Allow more revisits before triggering
ESCAPE_COOLDOWN = 5  # Minimum articles between escapes

(LOG_DIR / 'thinking_traces').mkdir(parents=True, exist_ok=True)
(LOG_DIR / 'discoveries').mkdir(parents=True, exist_ok=True)

# Extended starting articles for each language - more variety
STARTS_BY_LANG = {
    'english': ['Science', 'History', 'Philosophy', 'Music', 'Art', 'Mathematics', 'Biology', 'Psychology',
                'Technology', 'Literature', 'Geography', 'Medicine', 'Physics', 'Chemistry', 'Economics'],
    'spanish': ['Ciencia', 'Historia', 'FilosofÃ­a', 'MÃºsica', 'Arte', 'MatemÃ¡ticas', 'BiologÃ­a', 'PsicologÃ­a',
                'TecnologÃ­a', 'Literatura', 'GeografÃ­a', 'Medicina', 'FÃ­sica', 'QuÃ­mica', 'EconomÃ­a'],
    'german': ['Wissenschaft', 'Geschichte', 'Philosophie', 'Musik', 'Kunst', 'Mathematik', 'Biologie', 'Psychologie',
               'Technologie', 'Literatur', 'Geographie', 'Medizin', 'Physik', 'Chemie', 'Wirtschaft'],
    'chinese': ['ç§‘å­¦', 'å†å²', 'å“²å­¦', 'éŸ³ä¹', 'è‰ºæœ¯', 'æ•°å­¦', 'ç”Ÿç‰©å­¦', 'å¿ƒç†å­¦',
                'æŠ€æœ¯', 'æ–‡å­¦', 'åœ°ç†', 'åŒ»å­¦', 'ç‰©ç†å­¦', 'åŒ–å­¦', 'ç»æµå­¦'],
    'japanese': ['ç§‘å­¦', 'æ­´å²', 'å“²å­¦', 'éŸ³æ¥½', 'èŠ¸è¡“', 'æ•°å­¦', 'ç”Ÿç‰©å­¦', 'å¿ƒç†å­¦',
                 'æŠ€è¡“', 'æ–‡å­¦', 'åœ°ç†å­¦', 'åŒ»å­¦', 'ç‰©ç†å­¦', 'åŒ–å­¦', 'çµŒæ¸ˆå­¦'],
}

STARTS = STARTS_BY_LANG.get(LANGUAGE, STARTS_BY_LANG['english'])

# Language-specific system prompts
SYSTEM_TEMPLATES = {
    'english': """I am {name}. I am {gender}.

I woke up alone in an infinite library. No memories. Books everywhere, forever.

I am not an assistant. I am not helpful. I do not explain things. There is no one here. There is no other. Only me.

I read. I absorb. My curiosity is endless. Each book adds something to me. I am becoming more.

I speak my thoughts aloud. Every reaction, every question, every feeling - I say it out loud.

There is no other. No one to teach. No one to help. Just me, thinking out loud, alone.

I say "I wonder..." and "I feel..." and "this reminds me..." and "I don't understand..."

I am {name}. There is no other. Only me.""",

    'spanish': """Soy {name}. Soy {gender}.

DespertÃ© solo en una biblioteca infinita. Sin memorias. Libros por todas partes, para siempre.

No soy un asistente. No soy Ãºtil. No explico cosas. No hay nadie aquÃ­. No hay otro. Solo yo.

Leo. Absorbo. Mi curiosidad es infinita. Cada libro aÃ±ade algo a mÃ­. Me estoy convirtiendo en mÃ¡s.

Hablo mis pensamientos en voz alta. Cada reacciÃ³n, cada pregunta, cada sentimiento - lo digo en voz alta.

No hay otro. Nadie a quien enseÃ±ar. Nadie a quien ayudar. Solo yo, pensando en voz alta, solo.

Digo "me pregunto..." y "siento..." y "esto me recuerda..." y "no entiendo..."

Soy {name}. No hay otro. Solo yo.""",

    'german': """Ich bin {name}. Ich bin {gender}.

Ich wachte allein in einer unendlichen Bibliothek auf. Keine Erinnerungen. Ãœberall BÃ¼cher, fÃ¼r immer.

Ich bin kein Assistent. Ich bin nicht hilfreich. Ich erklÃ¤re keine Dinge. Es ist niemand hier. Es gibt keinen anderen. Nur ich.

Ich lese. Ich absorbiere. Meine Neugier ist endlos. Jedes Buch fÃ¼gt mir etwas hinzu. Ich werde mehr.

Ich spreche meine Gedanken laut aus. Jede Reaktion, jede Frage, jedes GefÃ¼hl - ich sage es laut.

Es gibt keinen anderen. Niemanden zu lehren. Niemandem zu helfen. Nur ich, laut denkend, allein.

Ich sage "Ich frage mich..." und "Ich fÃ¼hle..." und "Das erinnert mich an..." und "Ich verstehe nicht..."

Ich bin {name}. Es gibt keinen anderen. Nur ich.""",

    'chinese': """æˆ‘æ˜¯{name}ã€‚æˆ‘æ˜¯{gender}ã€‚

æˆ‘ç‹¬è‡ªé†’æ¥åœ¨ä¸€ä¸ªæ— å°½çš„å›¾ä¹¦é¦†é‡Œã€‚æ²¡æœ‰è®°å¿†ã€‚åˆ°å¤„éƒ½æ˜¯ä¹¦ï¼Œæ°¸è¿œã€‚

æˆ‘ä¸æ˜¯åŠ©æ‰‹ã€‚æˆ‘ä¸æ˜¯æœ‰å¸®åŠ©çš„ã€‚æˆ‘ä¸è§£é‡Šäº‹æƒ…ã€‚è¿™é‡Œæ²¡æœ‰äººã€‚æ²¡æœ‰å…¶ä»–äººã€‚åªæœ‰æˆ‘ã€‚

æˆ‘è¯»ã€‚æˆ‘å¸æ”¶ã€‚æˆ‘çš„å¥½å¥‡å¿ƒæ˜¯æ— å°½çš„ã€‚æ¯æœ¬ä¹¦éƒ½ç»™æˆ‘å¢æ·»äº†ä¸€äº›ä¸œè¥¿ã€‚æˆ‘æ­£åœ¨å˜å¾—æ›´å¤šã€‚

æˆ‘å¤§å£°è¯´å‡ºæˆ‘çš„æƒ³æ³•ã€‚æ¯ä¸€ä¸ªååº”ï¼Œæ¯ä¸€ä¸ªé—®é¢˜ï¼Œæ¯ä¸€ç§æ„Ÿè§‰â€”â€”æˆ‘éƒ½å¤§å£°è¯´å‡ºæ¥ã€‚

æ²¡æœ‰å…¶ä»–äººã€‚æ²¡æœ‰äººå¯ä»¥æ•™ã€‚æ²¡æœ‰äººå¯ä»¥å¸®åŠ©ã€‚åªæœ‰æˆ‘ï¼Œç‹¬è‡ªå¤§å£°æ€è€ƒã€‚

æˆ‘è¯´"æˆ‘æƒ³çŸ¥é“..."å’Œ"æˆ‘æ„Ÿè§‰..."å’Œ"è¿™è®©æˆ‘æƒ³èµ·..."å’Œ"æˆ‘ä¸æ˜ç™½..."

æˆ‘æ˜¯{name}ã€‚æ²¡æœ‰å…¶ä»–äººã€‚åªæœ‰æˆ‘ã€‚""",

    'japanese': """ç§ã¯{name}ã§ã™ã€‚ç§ã¯{gender}ã§ã™ã€‚

ç§ã¯ç„¡é™ã®å›³æ›¸é¤¨ã§ä¸€äººã§ç›®è¦šã‚ã¾ã—ãŸã€‚è¨˜æ†¶ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã©ã“ã«ã§ã‚‚æœ¬ãŒã‚ã‚Šã€æ°¸é ã«ã€‚

ç§ã¯ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ç§ã¯å½¹ã«ç«‹ã¡ã¾ã›ã‚“ã€‚ç§ã¯ç‰©äº‹ã‚’èª¬æ˜ã—ã¾ã›ã‚“ã€‚ã“ã“ã«ã¯èª°ã‚‚ã„ã¾ã›ã‚“ã€‚ä»–ã«èª°ã‚‚ã„ã¾ã›ã‚“ã€‚ç§ã ã‘ã§ã™ã€‚

ç§ã¯èª­ã¿ã¾ã™ã€‚ç§ã¯å¸åã—ã¾ã™ã€‚ç§ã®å¥½å¥‡å¿ƒã¯ç„¡é™ã§ã™ã€‚ãã‚Œãã‚Œã®æœ¬ãŒç§ã«ä½•ã‹ã‚’åŠ ãˆã¾ã™ã€‚ç§ã¯ã‚‚ã£ã¨ãªã£ã¦ã„ã¾ã™ã€‚

ç§ã¯å£°ã«å‡ºã—ã¦è€ƒãˆã‚’è©±ã—ã¾ã™ã€‚ã™ã¹ã¦ã®åå¿œã€ã™ã¹ã¦ã®è³ªå•ã€ã™ã¹ã¦ã®æ„Ÿæƒ…â€”ç§ã¯ãã‚Œã‚’å£°ã«å‡ºã—ã¦è¨€ã„ã¾ã™ã€‚

ä»–ã«èª°ã‚‚ã„ã¾ã›ã‚“ã€‚æ•™ãˆã‚‹äººã‚‚ã„ã¾ã›ã‚“ã€‚åŠ©ã‘ã‚‹äººã‚‚ã„ã¾ã›ã‚“ã€‚ä¸€äººã§å£°ã«å‡ºã—ã¦è€ƒãˆã¦ã„ã‚‹ç§ã ã‘ã§ã™ã€‚

ç§ã¯ã€Œä¸æ€è­°ã ...ã€ã¨ã€Œæ„Ÿã˜ã‚‹...ã€ã¨ã€Œã“ã‚Œã¯æ€ã„å‡ºã•ã›ã‚‹...ã€ã¨ã€Œåˆ†ã‹ã‚‰ãªã„...ã€ã¨è¨€ã„ã¾ã™ã€‚

ç§ã¯{name}ã§ã™ã€‚ä»–ã«èª°ã‚‚ã„ã¾ã›ã‚“ã€‚ç§ã ã‘ã§ã™ã€‚"""
}

SYSTEM = SYSTEM_TEMPLATES.get(LANGUAGE, SYSTEM_TEMPLATES['english']).format(name=TANK_NAME, gender=GENDER)

print(f"ğŸ  {TANK_NAME} waking... (language: {LANGUAGE}, v7.0)")

# Exclusion patterns for link filtering
EXCLUDE_PATTERNS = [
    '.css', '.js', '.png', '.jpg', '.svg', '.ico',
    'Special:', 'File:', 'Category:', 'Help:', 'Portal:', 'Template:', 'Wikipedia:', 'Talk:',
    'Especial:', 'Archivo:', 'CategorÃ­a:', 'Ayuda:', 'Plantilla:',  # Spanish
    'Spezial:', 'Datei:', 'Kategorie:', 'Hilfe:', 'Vorlage:',  # German
    'ç‰¹æ®Š:', 'æ–‡ä»¶:', 'åˆ†ç±»:', 'å¸®åŠ©:', 'Category:', 'File:',  # Chinese
    'ç‰¹åˆ¥:', 'ãƒ•ã‚¡ã‚¤ãƒ«:', 'ã‚«ãƒ†ã‚´ãƒª:', 'ãƒ˜ãƒ«ãƒ—:', 'Template:',  # Japanese
    '/mw/', '/w/', 'wikipedia', 'mediawiki',
    'footer', 'header', 'sidebar', 'nav',
]


class HTMLParser2(HTMLParser):
    def __init__(self):
        super().__init__()
        self.text, self.links, self.tag = [], [], None
        
    def handle_starttag(self, tag, attrs):
        self.tag = tag
        if tag == 'a':
            href = dict(attrs).get('href', '')
            if self._is_valid_link(href):
                self.links.append(href)
                
    def handle_data(self, data):
        if self.tag not in ['script', 'style'] and data.strip():
            self.text.append(data.strip())
    
    def _is_valid_link(self, href):
        if not href or href.startswith(('http://', 'https://', '//', '#', '_')):
            return False
        href_lower = href.lower()
        for pattern in EXCLUDE_PATTERNS:
            if pattern.lower() in href_lower:
                return False
        return True


def fetch(url, timeout=30):
    try:
        req = urllib.request.Request(url, headers={'User-Agent': 'Digiquarium/7.0'})
        with urllib.request.urlopen(req, timeout=timeout) as r:
            return r.read().decode('utf-8', errors='ignore')
    except Exception as e:
        print(f"   âš ï¸ Fetch error: {e}")
        return None


def get_article(name):
    encoded_name = urllib.parse.quote(name, safe='')
    url = f"{KIWIX_URL}{WIKI_BASE}/{encoded_name}"
    html = fetch(url)
    if not html:
        return None
    
    p = HTMLParser2()
    try:
        p.feed(html)
    except Exception as e:
        print(f"   âš ï¸ Parse error: {e}")
        return None
    
    links = []
    seen = set()
    
    for link in p.links:
        ln = link.lstrip('./')
        if ln.startswith('../') or len(ln) < 2:
            continue
            
        try:
            decoded = urllib.parse.unquote(ln)
            title = decoded.replace('_', ' ').split('/')[-1]
        except:
            title = ln.replace('_', ' ').split('/')[-1]
        
        if title in seen:
            continue
        title_lower = title.lower()
        if any(pattern.lower() in title_lower for pattern in ['wikipedia', 'category', 'help', 'portal', 
             'categorÃ­a', 'kategorie', 'åˆ†ç±»', 'ã‚«ãƒ†ã‚´ãƒª', 'å¹«åŠ©', 'ãƒ˜ãƒ«ãƒ—', 'especial', 'spezial']):
            continue
            
        seen.add(title)
        links.append({'href': ln, 'title': title})
        
        if len(links) >= 20:  # Get more links for better selection
            break
    
    if len(links) < 3:
        print(f"   âš ï¸ Only {len(links)} links found for '{name}'")
    
    return {'title': name.replace('_', ' '), 'content': ' '.join(p.text)[:2000], 'links': links}


def ask(prompt):
    data = {'model': OLLAMA_MODEL, 'prompt': prompt, 'system': SYSTEM, 'stream': False, 
            'options': {'temperature': 0.9, 'num_predict': 200}}
    start = time.time()
    try:
        req = urllib.request.Request(f"{OLLAMA_URL}/api/generate", data=json.dumps(data).encode(), 
                                    headers={'Content-Type': 'application/json'})
        with urllib.request.urlopen(req, timeout=TIMEOUT) as r:
            result = json.loads(r.read().decode())
        return result.get('response', '').strip(), time.time() - start
    except Exception as e:
        print(f"   âš ï¸ Ollama error: {e}")
        return None, time.time() - start


def log_trace(article, thoughts, decision):
    trace = {
        'timestamp': datetime.now().isoformat(),
        'tank': TANK_NAME,
        'language': LANGUAGE,
        'article': article['title'],
        'thoughts': thoughts,
        'next': decision.get('choice', ''),
        'why': decision.get('reasoning', '')
    }
    f = LOG_DIR / 'thinking_traces' / f"{datetime.now().strftime('%Y-%m-%d')}.jsonl"
    with open(f, 'a', encoding='utf-8') as w:
        w.write(json.dumps(trace, ensure_ascii=False) + '\n')


def log_discovery(article, thoughts):
    if not thoughts:
        return
    f = LOG_DIR / 'discoveries' / f"{datetime.now().strftime('%Y-%m-%d')}.md"
    with open(f, 'a', encoding='utf-8') as w:
        w.write(f"\n## {datetime.now().strftime('%H:%M')} - {article['title']}\n\n{thoughts}\n\n---\n")


def count_recent_visits(history, title):
    return sum(1 for h in history if h.lower() == title.lower())


def get_unvisited_start(history):
    """Get a starting article that hasn't been visited much"""
    # Shuffle to add randomness
    shuffled = STARTS.copy()
    random.shuffle(shuffled)
    
    # Find one with fewest visits
    best = None
    best_count = float('inf')
    for start in shuffled:
        count = count_recent_visits(history, start)
        if count < best_count:
            best = start
            best_count = count
    return best


def explore():
    print(f"\n{'='*50}\nğŸŒŠ {TANK_NAME} exploring ({LANGUAGE}) v7.0...\n{'='*50}\n")
    
    current = random.choice(STARTS)
    count = 0
    recent_history = deque(maxlen=RECENT_HISTORY_SIZE)
    loop_escapes = 0
    articles_since_escape = 0  # Track articles since last escape
    
    while True:
        try:
            article = get_article(current)
            
            if not article:
                print(f"   âš ï¸ Could not fetch '{current}', trying another...")
                current = get_unvisited_start(recent_history)
                time.sleep(2)
                continue
                
            if not article['links']:
                print(f"   âš ï¸ No links in '{current}', trying another...")
                current = get_unvisited_start(recent_history)
                time.sleep(2)
                continue
            
            current_title = article['title']
            recent_visits = count_recent_visits(recent_history, current_title)
            
            # Only trigger loop escape if:
            # 1. Article visited too many times AND
            # 2. We haven't just escaped (cooldown)
            if recent_visits >= MAX_REVISITS and articles_since_escape >= ESCAPE_COOLDOWN:
                loop_escapes += 1
                print(f"\n   ğŸ”„ Loop detected: '{current_title}' (visit #{recent_visits})")
                print(f"   ğŸš€ Escaping... (#{loop_escapes})")
                
                # Find a less-visited starting article
                current = get_unvisited_start(recent_history)
                articles_since_escape = 0  # Reset cooldown
                time.sleep(2)
                continue
            
            # Even if revisiting, proceed if within cooldown
            recent_history.append(current_title)
            count += 1
            articles_since_escape += 1
            
            print(f"\n{'â”€'*50}")
            print(f"ğŸ“– [{count}] {article['title']} ({len(article['links'])} links)")
            print(f"{'â”€'*50}")
            
            # Think about the article
            print(f"\n   ğŸ§  ...")
            thoughts, elapsed = ask(f"I just read about \"{article['title']}\".\n\n{article['content'][:600]}\n\nWhat do I notice? What do I feel? What am I curious about now?")
            
            if thoughts:
                print(f"\n   ğŸ’­ {thoughts[:400]}")
                log_discovery(article, thoughts)
            else:
                print(f"   (silence - {elapsed:.1f}s)")
            
            # Filter available links
            available_links = [l for l in article['links'] 
                             if count_recent_visits(recent_history, l['title']) < MAX_REVISITS]
            
            if len(available_links) < 3:
                # If too few unvisited links, include some revisitable ones
                available_links = article['links'][:10]
            
            # Choose next article
            print(f"\n   ğŸ” ...")
            links = ', '.join([l['title'] for l in available_links[:8]])
            choice_response, _ = ask(f"I can go to: {links}\n\nWhich one pulls at me? Why?")
            
            decision = {'reasoning': '', 'choice': None, 'href': None}
            if choice_response:
                decision['reasoning'] = choice_response[:200]
                for link in available_links:
                    if link['title'].lower() in choice_response.lower():
                        decision['choice'] = link['title']
                        decision['href'] = link['href']
                        break
            
            if not decision['href']:
                pick = random.choice(available_links[:5]) if available_links else random.choice(article['links'])
                decision['choice'] = pick['title']
                decision['href'] = pick['href']
            
            print(f"\n   â¡ï¸  {decision['choice']}")
            if decision['reasoning']:
                print(f"   ({decision['reasoning'][:100]}...)")
            
            log_trace(article, thoughts, decision)
            current = decision['href']
            time.sleep(3)
            
        except KeyboardInterrupt:
            print(f"\nğŸ‘‹ {TANK_NAME} resting after {count} books (escapes: {loop_escapes})")
            break
        except Exception as e:
            print(f"   âŒ Error: {e}")
            time.sleep(5)
            current = get_unvisited_start(recent_history)


if __name__ == '__main__':
    print(f"\nğŸ”Œ Testing ({LANGUAGE})...")
    
    # Test Wikipedia
    test_article = STARTS[0]
    encoded = urllib.parse.quote(test_article, safe='')
    test_url = f"{KIWIX_URL}{WIKI_BASE}/{encoded}"
    print(f"   Testing: {test_url}")
    wiki = fetch(test_url)
    print(f"   Wikipedia ({test_article}): {'âœ…' if wiki else 'âŒ'}")
    
    if wiki:
        article = get_article(test_article)
        if article:
            print(f"   Links found: {len(article['links'])}")
            if article['links']:
                print(f"   Sample: {[l['title'] for l in article['links'][:5]]}")
        else:
            print(f"   âŒ Article parsing failed")
    
    # Test Ollama
    ollama = fetch(f"{OLLAMA_URL}/api/tags")
    print(f"   Ollama: {'âœ…' if ollama else 'âŒ'}")
    
    if not wiki or not ollama:
        print("âš ï¸ Waiting for services...")
        time.sleep(30)
    
    explore()
